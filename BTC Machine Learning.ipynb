{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First things first. We need to import all the needed [modules](https://docs.python.org/3/reference/import.html) for this project. Below, I've made a list and described each module and included a link for its website. Typically, to follow along, you just need to `pip install` any of these (for example, `pip install matplotlib`), unless you are using a version of [Anaconda](https://www.anaconda.com/), in which case, you can use `conda install` for any of these modules.\n",
    "\n",
    "# Modules\n",
    "- [Numpy](http://www.numpy.org/) is an incredibly powerful tool that is used almost everywhere you encounter matrices in Python. We also use it mainly for its powerful operations on their N-dimensional array objects.\n",
    "\n",
    "- [Pandas](https://pandas.pydata.org/) is a must for data science in Python. It handles powerful, expressive, and flexible data structures that allow easy data manipulation and analysis (among other things). Main thing to take away are the DataFrames.\n",
    "- [StatsModels](https://www.statsmodels.org/stable/index.html) is a Python module that provides classes and functions for the estimation of many different statistical models, as well as for conducting statistical tests, and statistical data exploration. It complements SciPy computations providing descriptive statistics and estimation and inference for statistical models.\n",
    "- [SciPy](https://www.scipy.org/) contains modules for optimization, linear algebra, integration, interpoliation, and much more. Here we will use it mainly for _______.\n",
    "- [SkLearn](http://scikit-learn.org/stable/) is a Python module for machine learning built on top of SciPy \n",
    "- [Math](https://docs.python.org/3/library/math.html) is avaiable by default (i.e. no need to install separately!). This module includes common mathematical functions.\n",
    "- [Random](https://docs.python.org/3/library/random.html) is a module that generates psuedo-random numbers from various distributions. \n",
    "- [Keras](https://keras.io/) is an open source neural network library.\n",
    "- [Matplotlib](https://matplotlib.org/) is a plotting library. If you're familiar with MATLAB, then this should be familiar as well.\n",
    "- [Plotly](https://plot.ly/python/) is an interactive, open-source, and browser-based graphing library for Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from random import randint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import GRU\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import initializers\n",
    "from matplotlib import pyplot\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "py.init_notebook_mode(connected=True)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Now we load up the data and do a little bit of data wrangling. We will load the data into a pandas dataframe and make sure of the following:\n",
    "1. there are no nulls (if there are, we can figure out what to do then),\n",
    "2. the format of the data is acceptable (i.e. datetime)\n",
    "3. bin data as necessary (data will probably come to us in minute/hourly data given the nature of the machine-generated financial data) so we will need to bin the data (for now, we will go with daily updates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load up the data into pandas dataframe called data\n",
    "data = pd.read_csv('./data/bitstampUSD_1-min_data_2012-01-01_to_2018-06-27.csv')\n",
    "\n",
    "# We can double check the type of data. It should be dataframe.\n",
    "type(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!\n",
    "\n",
    "Data is loaded into `data`. Note that since we used `pd` (i.e. `pandas`), data is a pandas DataFrame.\n",
    "\n",
    "As with any data science project, we need to get familar with our data. Let's take a **(1) look at the data** and **(2) clean the data as needed**.\n",
    "- Check to see if any values are null in the dataframe. Keep in mind, we are looking at financial data that is machine generated, so we're unlikely to have any null values. If we do have any, it would be easy to fill in the gaps using a simple interpolation, but we'll cross that bridge if we have to.\n",
    "- Take a look at the top 10 rows to look at the value and get a sense of what the data looks like.\n",
    "- Scale data as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are any data points null?\n",
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does the first 10 rows look like?\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take note of a few things:\n",
    "\n",
    "1. Take a look at the timestamp. The number you see is a Unix timestamp that captures the number of seconds since epoch (considered to be 1970-01-01 12AM). \n",
    "Take for instance, the first row (1325317980). That is equivalent to Saturday, December 31, 2011 7:52:00 AM (GMT) and the second row (1325317980) is the same as Saturday, December 31, 2011 7:53:00 AM. Mainly, notice that it is in the same day. So for the time being, we will look at data partitioned by day, so we're going to need to get the average price grouped by the day.\n",
    "2. We want to change the timestamp into a human readable datetime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we create a new column called \"date\" then we group by and grab the average\n",
    "data['date'] = pd.to_datetime(data['Timestamp'],unit='s').dt.date\n",
    "group = data.groupby('date')\n",
    "Daily_Price = group['Weighted_Price'].mean()\n",
    "\n",
    "# let's take a look at the first 5\n",
    "Daily_Price.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#and the last 5\n",
    "Daily_Price.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split our dataset\n",
    "\n",
    "We need to **split our dataset** because we want to train and test the model only on some chunk of the data. So, in the next cell, we are counting the necessary parameters for splitting (number of days between some dates). We want to train our model on the data from January 1, 2016, until March 1, 2018, and to test the model on the data from March 1, 2018, until June 27, 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "d0 = date(2016, 1, 1)\n",
    "d1 = date(2018, 6, 27)\n",
    "delta = d1 - d0\n",
    "days_look = delta.days + 1\n",
    "print(\"days between\", d0, \"and\", d1, \":\", days_look)\n",
    "\n",
    "d0 = date(2018, 3, 1)\n",
    "d1 = date(2018, 6, 27)\n",
    "delta = d1 - d0\n",
    "days_from_train = delta.days + 1\n",
    "print(\"days between\", d0, \"and\", d1, \":\", days_from_train)\n",
    "\n",
    "d0 = date(2018, 6, 27)\n",
    "d1 = date(2018, 6, 27)\n",
    "delta = d1 - d0\n",
    "days_from_end = delta.days + 1\n",
    "print(\"days between\", d0, \"and\", d1, \":\", days_from_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we are splitting our data into the train and test set:\n",
    "\n",
    "df_train= Daily_Price[len(Daily_Price)-days_look-days_from_end:len(Daily_Price)-days_from_train]\n",
    "df_test= Daily_Price[len(Daily_Price)-days_from_train:]\n",
    "\n",
    "# Just as a sanity-check, \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('./data/df_train.csv')\n",
    "df_test.to_csv('./data/df_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cursory Data Analysis\n",
    "\n",
    "We want to estimate some parameters of our data because this can be useful in the further model designing. The first important thing when forecasting time series is to check if the data is stationary. This means that our data is influenced by such factors as trend or seasonality.\n",
    "\n",
    "In the next cell, we concatenate train and test data to make analysis and transformations simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_data = [df_train, df_test]\n",
    "working_data = pd.concat(working_data)\n",
    "\n",
    "working_data = working_data.reset_index()\n",
    "working_data['date'] = pd.to_datetime(working_data['date'])\n",
    "working_data = working_data.set_index('date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seasonal Decomposition\n",
    "\n",
    "Next, we will perform a seasonal decomposition on the data (inevitable given the breadth and category of the data we are workign with) to estimate its trend and seasonality. We will plot it all out so we can see the actual price movements on the plot below (“observed”) as well as the trend and seasonality in our data.\n",
    "\n",
    "We will be using `seasonal_decompose` function which is a part of the `time series analysis (tsa)` in `statsmodel`. Click [here](https://www.statsmodels.org/dev/generated/statsmodels.tsa.seasonal.seasonal_decompose.html) to read more about it.\n",
    "\n",
    "Further, as mentioned above, we will be using plot.ly's line and scatter plots, which you can read more about [here](https://plot.ly/python/line-and-scatter/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = sm.tsa.seasonal_decompose(working_data.Weighted_Price.values, freq=60)\n",
    "\n",
    "trace1 = go.Scatter(x = np.arange(0, len(s.trend), 1),\n",
    "                    y = s.trend,mode = 'lines',\n",
    "                    name = 'Trend',\n",
    "                    line = dict(color = ('rgb(244, 93, 1)'), width = 2)\n",
    "                   )\n",
    "\n",
    "trace2 = go.Scatter(x = np.arange(0, len(s.seasonal), 1),\n",
    "                    y = s.seasonal,mode = 'lines',\n",
    "                    name = 'Seasonal',\n",
    "                    line = dict(color = ('rgb(238, 185, 2)'),width = 2)\n",
    "                   )\n",
    "\n",
    "trace3 = go.Scatter(x = np.arange(0, len(s.resid), 1),\n",
    "                    y = s.resid,mode = 'lines',\n",
    "                    name = 'Residual',\n",
    "                    line = dict(color = ('rgb(151, 204, 4)'), width = 2))\n",
    "\n",
    "trace4 = go.Scatter(x = np.arange(0, len(s.observed), 1),\n",
    "                    y = s.observed,mode = 'lines',\n",
    "                    name = 'Observed',\n",
    "                    line = dict(color = ('rgb(45, 125, 210)'), width = 2))\n",
    "\n",
    "data = [trace1, trace2, trace3, trace4]\n",
    "\n",
    "layout = dict(title = 'Seasonal decomposition',\n",
    "              xaxis = dict(title = 'Time'),\n",
    "              yaxis = dict(title = 'Price, USD')\n",
    "             )\n",
    "\n",
    "fig = dict(data=data,\n",
    "           layout=layout)\n",
    "\n",
    "py.iplot(fig, filename='seasonal_decomposition')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autocorrelation\n",
    "\n",
    "Next, we need to look at the [autocorrelation](https://en.wikipedia.org/wiki/Autocorrelation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "\n",
    "ax = plt.subplot(211)\n",
    "sm.graphics.tsa.plot_acf(working_data.Weighted_Price.values.squeeze(),\n",
    "                         lags=48,\n",
    "                         ax=ax)\n",
    "\n",
    "ax = plt.subplot(212)\n",
    "sm.graphics.tsa.plot_pacf(working_data.Weighted_Price.values.squeeze(),\n",
    "                          lags=48,\n",
    "                          ax=ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to recover our df_train and df_test datasets:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = working_data[:-60]\n",
    "df_test = working_data[-60:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "We need to prepare our dataset according to the requirements of the model, as well as to split the dataset into train and test parts. In the next cell, we define a function which creates X inputs and Y labels for our model. In the sequential forecasting, we predict the future value based on some previous and current values. So, our Y label is the value from the next (future) point of time while the X inputs are one or several values from the past. The amount of these values we can set by tuning the parameter look_back in our function. If we set it to 1, this means that we predict current value t based on the previous value (t-1).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lookback(dataset, look_back=1):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(dataset) - look_back):\n",
    "        a = dataset[i:(i + look_back), 0]\n",
    "        X.append(a)\n",
    "        Y.append(dataset[i + look_back, 0])\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we perform final data preparation:\n",
    "\n",
    "Reshape the train and test datasets according to the requirements of the model.\n",
    "Scale the dataset by using the MinMaxScaler because LSTM models are scale sensitive.\n",
    "Apply our create_lookback function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "training_set = df_train.values\n",
    "print(training_set)\n",
    "training_set = np.reshape(training_set, (len(training_set), 1))\n",
    "test_set = df_test.values\n",
    "test_set = np.reshape(test_set, (len(test_set), 1))\n",
    "\n",
    "#scale datasets\n",
    "scaler = MinMaxScaler()\n",
    "training_set = scaler.fit_transform(training_set)\n",
    "test_set = scaler.transform(test_set)\n",
    "\n",
    "# create datasets which are suitable for time series forecasting\n",
    "look_back = 1\n",
    "X_train, Y_train = create_lookback(training_set, look_back)\n",
    "X_test, Y_test = create_lookback(test_set, look_back)\n",
    "\n",
    " # reshape datasets so that they will be ok for the requirements of the LSTM model in Keras\n",
    "X_train = np.reshape(X_train, (len(X_train), 1, X_train.shape[1]))\n",
    "X_test = np.reshape(X_test, (len(X_test), 1, X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training 2-layers LSTM Neural Network\n",
    "\n",
    "Eventually, we can build and train our model. We use Keras framework for deep learning. Our model consists of two stacked LSTM layers with 256 units each and the densely connected output layer with one neuron. We are using Adam optimizer and MSE as a loss. Also, we use an early stopping if the result doesn't improve during 20 training iterations (epochs). We performed several experiments and found that the optimal number of epochs and butch_size is 100 and 16 respectively. Also, it is important to set shuffle=False because we don't want to shuffle time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize sequential model, add 2 stacked LSTM layers and densely connected output neuron\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, \n",
    "               return_sequences=True, \n",
    "               input_shape=(X_train.shape[1], \n",
    "                            X_train.shape[2]\n",
    "                           )\n",
    "              )\n",
    "         )\n",
    "model.add(LSTM(256))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# compile and fit the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "history = model.fit(X_train,\n",
    "                    Y_train, \n",
    "                    epochs=100, \n",
    "                    batch_size=16, \n",
    "                    shuffle=False, \n",
    "                    validation_data=(X_test, Y_test), \n",
    "                    callbacks = [EarlyStopping(monitor='val_loss', min_delta=5e-5, patience=20, verbose=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace1 = go.Scatter(\n",
    "    x = np.arange(0, len(history.history['loss']), 1),\n",
    "    y = history.history['loss'],\n",
    "    mode = 'lines',\n",
    "    name = 'Train loss',\n",
    "    line = dict(color=('rgb(66, 244, 155)'), width=2, dash='dash')\n",
    ")\n",
    "trace2 = go.Scatter(\n",
    "    x = np.arange(0, len(history.history['val_loss']), 1),\n",
    "    y = history.history['val_loss'],\n",
    "    mode = 'lines',\n",
    "    name = 'Test loss',\n",
    "    line = dict(color=('rgb(244, 146, 65)'), width=2)\n",
    ")\n",
    "\n",
    "data = [trace1, trace2]\n",
    "layout = dict(title = 'Train and Test Loss during training',\n",
    "              xaxis = dict(title = 'Epoch number'), yaxis = dict(title = 'Loss'))\n",
    "fig = dict(data=data, layout=layout)\n",
    "py.iplot(fig, filename='training_process')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We trained our model. You can see that it has good performance even after several iterations. On the plot above, we compare the Train and Test loss on each iteration of the training process. We can see, that after some iterations train and test loss become very similar, which is a good sign (this means we are not overfitting the train set). Below, we use our model to predict labels for the test set. Then we inverse original scale of our data. You can see a comparison of true and predicted labels on the chart below. It looks like our model gives good results (lines are very similar)!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## THIS IS A TEST HERE WHERE I PRINT STUFF OUT\n",
    "\n",
    "\n",
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add one additional data point to align shapes of the predictions and true labels\n",
    "X_test = np.append(X_test, scaler.transform(working_data.iloc[-1][0].reshape(-1,1)))\n",
    "X_test = np.reshape(X_test, (len(X_test), 1, 1))\n",
    "\n",
    "# get predictions and then make some transformations to be able to calculate RMSE properly in USD\n",
    "prediction = model.predict(X_test)\n",
    "prediction_inverse = scaler.inverse_transform(prediction.reshape(-1, 1))\n",
    "Y_test_inverse = scaler.inverse_transform(Y_test.reshape(-1, 1))\n",
    "prediction2_inverse = np.array(prediction_inverse[:,0][1:])\n",
    "Y_test2_inverse = np.array(Y_test_inverse[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trace1 = go.Scatter(\n",
    "    x = np.arange(0, len(prediction2_inverse), 1),\n",
    "    y = prediction2_inverse,\n",
    "    mode = 'lines',\n",
    "    name = 'Predicted labels',\n",
    "    line = dict(color=('rgb(244, 146, 65)'), width=2)\n",
    ")\n",
    "trace2 = go.Scatter(\n",
    "    x = np.arange(0, len(Y_test2_inverse), 1),\n",
    "    y = Y_test2_inverse,\n",
    "    mode = 'lines',\n",
    "    name = 'True labels',\n",
    "    line = dict(color=('rgb(66, 244, 155)'), width=2)\n",
    ")\n",
    "\n",
    "data = [trace1, trace2]\n",
    "layout = dict(title = 'Comparison of true prices (on the test dataset) with prices our model predicted',\n",
    "             xaxis = dict(title = 'Day number'), yaxis = dict(title = 'Price, USD'))\n",
    "fig = dict(data=data, layout=layout)\n",
    "py.iplot(fig, filename='results_demonstrating0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
